{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/4147067676.py:23: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,KFold,RandomizedSearchCV,GridSearchCV\n",
    "from sklearn import metrics,svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/haochunniu/Desktop/Kaggle Compatition/Credit Card Customer Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement #\n",
    "As my master education completed and no longer a student, the very first financial tool I obtained is credit card. In nowadays business environment, bank segments customers and provides all kinds of credit card. Different types of credit card provide different levels of benefits and advantages. Hence, to understand what kind of benefit I could receive based on my income and basic information, I decide to create a classification model to predict the level of card customer would received.  \n",
    "     \n",
    "In this project, I will try several different models, such as NN, XGBoost, and LGBM. In addition, to compare the performance of different models and find the best hyperparameter combination, I will use Nested Cross-Validation, Cross-Validation, Grid-Search, and Random-Search techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "Within the dataset, there are totally **10,127** rows and **23** columns. **6** of the columns are categorical variables, and the rest of the columns are all numeric columns. The **final independent variable** we are interested in is the **Card Category** categorical variable. It only has **4 possible outcomes, Blue, Silver, Gold, and Platium**. Fortunately, the data is quite clean and tidy. There is no NA or NULL value within the data. Among all the columns,  Avg_Open_To_Buy column does not have clear definition and explanation, and I am not sure about how the number is calculated. Hence, the column is eventually dropped from the data. Also, becuase the CLIENTNUM column does not have any actual meaning, it is also droppped. In addition, customer income is originally divided into several categories. To make it more precise and comparable, I turn the categorical income feature into numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"BankChurners.csv\")\n",
    "raw = raw.drop(columns=\"Avg_Open_To_Buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_mapping={'Less than $40K':20000,\n",
    "                '$40K - $60K':50000,\n",
    "                '$60K - $80K':70000,\n",
    "                '$80K - $120K':100000,\n",
    "                '$120K +':200000,\n",
    "                'Unknown':80000\n",
    "                }\n",
    "raw=raw.assign(Income=raw.Income_Category.map(income_mapping))\n",
    "raw=raw.drop(columns=[\"Income_Category\",\"CLIENTNUM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution of the card categories, this data set suffered from extreme **imbalance data** issue. I will eventually fix this issue with **sampling techniques (under/over-sampling)** or **setting specific class weight**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blue        93.18\n",
       "Silver       5.48\n",
       "Gold         1.15\n",
       "Platinum     0.20\n",
       "Name: Card_Category, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage of 4 card categories\n",
    "round(raw.Card_Category.value_counts()/len(raw)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis & Data Visualization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, all of the EDA and Data Visualization will be done on Tableau. Please use the link below to access the public dashboard on my Tableau public account.   \n",
    "https://public.tableau.com/views/CreditCardCustomerSegmentationAnalysis/Dashboard1?:language=en-US&publish=yes&:display_count=n&:origin=viz_share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Data Split ##\n",
    "To evaluate final model's performance and train models, I split the entire dataset into a **train (80%) and a test (20%)** dataset. To tune the **hyper-parameters and choose the best performing model**, I will use **cross validation** and **nested cross validation** techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(raw.drop(columns=\"Card_Category\"),raw.Card_Category,test_size=0.2,random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card category distribution for train data\n",
      "Blue        93.28\n",
      "Silver       5.39\n",
      "Gold         1.11\n",
      "Platinum     0.21\n",
      "Name: Card_Category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage of 4 card categories for train data\n",
    "print(\"Card category distribution for train data\")\n",
    "print(round(y_train.value_counts()/len(y_train)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card category distribution for test data\n",
      "Blue        92.74\n",
      "Silver       5.82\n",
      "Gold         1.28\n",
      "Platinum     0.15\n",
      "Name: Card_Category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage of 4 card categories for test data\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "print(\"Card category distribution for test data\")\n",
    "print(round(y_test.value_counts()/len(y_test)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection: Isolation Forest ##\n",
    "Before we start to deal with imbalance data, I first handel the outlier with **the Isolation Forest method**. In addition, bacuase the data is extremely imbalanced, I will **detect outliers category by category**, instead of detecting outliers of the entire training data. With the Isolatio Forest function, **the outliers** will be tagged as **-1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "model_IF = IsolationForest(contamination=float(0.05),random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['anomaly'] = model_IF.predict(x_train_tem_encode.values)\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['Card_Category']=i\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['anomaly'] = model_IF.predict(x_train_tem_encode.values)\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['Card_Category']=i\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['anomaly'] = model_IF.predict(x_train_tem_encode.values)\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['Card_Category']=i\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['anomaly'] = model_IF.predict(x_train_tem_encode.values)\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_3579/289527088.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tem['Card_Category']=i\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "for i in y_train.unique():\n",
    "    x_train_tem = x_train[y_train==i]\n",
    "    x_train_tem_encode = pd.get_dummies(x_train_tem,columns=['Attrition_Flag','Gender','Education_Level','Marital_Status'])\n",
    "    model_IF.fit(x_train_tem_encode.values)\n",
    "    train_tem=x_train_tem.iloc[:,:]\n",
    "    train_tem['anomaly'] = model_IF.predict(x_train_tem_encode.values)\n",
    "    train_tem['Card_Category']=i\n",
    "    if index==1:\n",
    "        train_no_out=train_tem\n",
    "    else:\n",
    "        train_no_out=pd.concat([train_no_out,train_tem])\n",
    "    index+=1\n",
    "\n",
    "train_no_out = train_no_out[train_no_out.anomaly==1]\n",
    "train_no_out = train_no_out.reset_index(drop=True)\n",
    "x_train_no_out = train_no_out.drop(columns=['Card_Category','anomaly'])\n",
    "y_train_no_out = train_no_out.Card_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card category distribution for train data without outliers\n",
      "Blue        93.29\n",
      "Silver       5.39\n",
      "Gold         1.10\n",
      "Platinum     0.21\n",
      "Name: Card_Category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage of 4 card categories for train data without outliers\n",
    "print(\"Card category distribution for train data without outliers\")\n",
    "print(round(y_train_no_out.value_counts()/len(y_train_no_out)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding Categorical Variable ##   \n",
    "Before fitting the model, I will transform the categorical variable into numeric variable via **One-Hot Encoding method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot encode X\n",
    "dummy_x_train_no_out = pd.get_dummies(x_train_no_out,columns=['Attrition_Flag','Gender','Education_Level','Marital_Status'])\n",
    "dummy_x_test =  pd.get_dummies(x_test,columns=['Attrition_Flag','Gender','Education_Level','Marital_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot eoncde Y\n",
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(y_train_no_out)\n",
    "encoded_y_train_no_out = encoder1.transform(y_train_no_out)\n",
    "dummy_y_train_no_out = np_utils.to_categorical(encoded_y_train_no_out)\n",
    "single_label_y_train_no_out=np.argmax(dummy_y_train_no_out, axis=1)\n",
    "\n",
    "encoder2 = LabelEncoder()\n",
    "encoder2.fit(y_test)\n",
    "encoded_y_test = encoder2.transform(y_test)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_y_test)\n",
    "single_label_y_test=np.argmax(dummy_y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Class Weight ##\n",
    "Given that the data is extremely imbalanced, I will first calculate the class weights for all credit card categories, before training and fitting the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class categories' weights: 0 is 'Blue', 1 is 'Gold', 2 'Platinum', 3 is 'Silver'\n",
    "nn_class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(y_train_no_out),y = y_train_no_out)\n",
    "nn_class_weights = dict(zip([0,1,2,3], nn_class_weights))\n",
    "\n",
    "nn_class_weights2 = dict(zip([0,1,2,3],[1,50,500,300]))\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(y_train_no_out),y = y_train_no_out)\n",
    "class_weights = dict(zip(np.unique(y_train_no_out), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike other models, XGBoost model use sample weights not class weights. So, this section is used to calculate sample weights\n",
    "xgb_class_weights = class_weight.compute_sample_weight(class_weight='balanced',y=y_train_no_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Random Search to select best ML model ##\n",
    "To find out the best model for the task, I use the **nested random search CV** technique to compare different models' performances. In this project, I try three different ML model, **XGBoost, LightGBM, and Random Forest**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Nested CV Weighted F1: 97.22 %\n",
      "Random Forest Nested CV Weighted F1: 93.6 %\n",
      "LightGBM Nested CV Weighted F1: 97.11 %\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the Classifier\n",
    "xgb=XGBClassifier(objective=\"multi:softmax\",seed=9,use_label_encoder =False)\n",
    "\n",
    "rf=RandomForestClassifier(random_state=9,class_weight=class_weights)\n",
    "\n",
    "lgbm=lgb.LGBMClassifier(objective='multiclass',random_state=9,class_weight=class_weights)\n",
    "\n",
    "##############################################################\n",
    "# 2. Create the parameter grid\n",
    "xgb_grid={'eta':np.arange(0.1,0.6,0.1),\n",
    "          'max_depth':list(range(3,16)),\n",
    "          'n_estimators':list(range(10,310,10)),\n",
    "          'gamma':list(range(1,6)) }\n",
    "\n",
    "rf_grid={'n_estimators':list(range(100,1100,100)),\n",
    "         'max_depth':list(range(3,11))}\n",
    "\n",
    "lgbm_grid={'learning_rate':np.arange(0.1,0.6,0.1),\n",
    "           'max_depth':list(range(3,16)),\n",
    "           'n_estimators':list(range(10,310,10))}\n",
    "\n",
    "##############################################################\n",
    "# 3. Create the CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "outer_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "\n",
    "##############################################################\n",
    "#In this case, because we are dealing with multi-class classification problem, we need to select a method to average our scoring metrics for the CVs\n",
    "#So, in this case, we use both weighted f1 for both outer and inner CV\n",
    "# 4-1-1. Random-search CV for XGBoost\n",
    "clf = RandomizedSearchCV(xgb,xgb_grid,cv=inner_cv,scoring='f1_weighted',n_iter=10,random_state=9)\n",
    "\n",
    "# 4-1-2. Nested CV for XGBoost\n",
    "nested_score = cross_val_score(clf,X=dummy_x_train_no_out, y=single_label_y_train_no_out, cv=outer_cv,scoring='f1_weighted') \n",
    "\n",
    "# 4-1-3. Result for Nested CV\n",
    "xgb_result=nested_score.mean()\n",
    "\n",
    "##############################################################\n",
    "# 4-2-1. Random-search CV for Random Forest\n",
    "clf = RandomizedSearchCV(rf,rf_grid,cv=inner_cv,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-2-2. Nested CV for Random Forest\n",
    "nested_score = cross_val_score(clf,X=dummy_x_train_no_out, y=y_train_no_out, cv=outer_cv,scoring='f1_weighted')\n",
    "\n",
    "# 4-2-3. Result for Nested CV\n",
    "rf_result=nested_score.mean()\n",
    "\n",
    "##############################################################\n",
    "# 4-3-1. Random-search CV for LightGBM Classifier\n",
    "clf = RandomizedSearchCV(lgbm,lgbm_grid,cv=inner_cv,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-3-2. Nested CV for LightGBM Classifier\n",
    "nested_score = cross_val_score(clf,X=dummy_x_train_no_out, y=y_train_no_out, cv=outer_cv,scoring='f1_weighted')\n",
    "\n",
    "# 4-3-3. Result for Nested CV\n",
    "lgbm_result=nested_score.mean()\n",
    "\n",
    "##############################################################\n",
    "print(\"XGBoost Nested CV Weighted F1:\",round(xgb_result*100,2),\"%\")\n",
    "print(\"Random Forest Nested CV Weighted F1:\",round(rf_result*100,2),\"%\")\n",
    "print(\"LightGBM Nested CV Weighted F1:\",round(lgbm_result*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search on XGBoost Classifier ##\n",
    "Based on nested CV result, XGBoost Classifier is the best performing model. To find out the best hyper-parameter combination, I use random search cross-validation to find the best performing hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create estimator\n",
    "xgb=XGBClassifier(objective=\"multi:softmax\",seed=9,use_label_encoder =False,verbosity = 0)\n",
    "\n",
    "# 2. Create parameter grid\n",
    "xgb_grid={'eta':np.arange(0.1,0.6,0.1),\n",
    "          'max_depth':list(range(3,16)),\n",
    "          'n_estimators':list(range(10,310,10)),\n",
    "          'gamma':list(range(1,6))}\n",
    "\n",
    "# 3. Grid-search\n",
    "xgb_model = RandomizedSearchCV(xgb,xgb_grid,cv=5,scoring='f1_weighted',n_iter=10,random_state=9)\n",
    "\n",
    "# 4. Fit the model\n",
    "xgb_model.fit(dummy_x_train_no_out,single_label_y_train_no_out,sample_weight=xgb_class_weights)\n",
    "\n",
    "# 5. Predict\n",
    "y_train_no_out_pred=xgb_model.predict(dummy_x_train_no_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With CV random search, I found the best hyperparameter is eta=0.4, max_depth=14, n_estimators=210, and gamma=1.\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      7179\n",
      "           1       0.89      1.00      0.94        85\n",
      "           2       1.00      1.00      1.00        16\n",
      "           3       0.88      1.00      0.94       415\n",
      "\n",
      "    accuracy                           0.99      7695\n",
      "   macro avg       0.94      1.00      0.97      7695\n",
      "weighted avg       0.99      0.99      0.99      7695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Result\n",
    "print (\"With CV random search, I found the best hyperparameter is eta={}, max_depth={}, n_estimators={}, and gamma={}.\".format(xgb_model.best_params_['eta'],\n",
    "                                                                                                                               xgb_model.best_params_['max_depth'],\n",
    "                                                                                                                               xgb_model.best_params_['n_estimators'],\n",
    "                                                                                                                               xgb_model.best_params_['gamma']))\n",
    "print('----------------------------------------------------------------------------------------------------------------')\n",
    "print(classification_report(single_label_y_train_no_out,y_train_no_out_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Final_XGBoost.pkl']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the final XGboost model\n",
    "import joblib\n",
    "joblib.dump(xgb_model.best_estimator_, 'Final_XGBoost.pkl')\n",
    "\n",
    "#Load the model\n",
    "#xgb_model = joblib.load(\"Final_XGBoost.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search on Neural Network ##\n",
    "After finding the best performing machine learning model, I also try some **deep learning models**. In this section, I will try both **3 layers** and **10 layers nerual network** models. To find the best hyper-parameters, I will use the **random search and train-validation split technique**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using cross-validation techniques, I just use simple 70-30 train, validation data split to evaluate different hyper-paramater combination performance.\n",
    "nn_dummy_x_train_no_out,nn_dummy_x_val_no_out,nn_dummy_y_train_no_out,nn_dummy_y_val_no_out = train_test_split(dummy_x_train_no_out,dummy_y_train_no_out,test_size=0.3,random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 03m 16s]\n",
      "val_categorical_accuracy: 0.9385015368461609\n",
      "\n",
      "Best val_categorical_accuracy So Far: 0.9444781422615052\n",
      "Total elapsed time: 00h 15m 06s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Random Search on 3 Layers Neural Network\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=200,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid']),\n",
    "                           input_dim=nn_dummy_x_train_no_out.shape[1]))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=200,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=200,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(4,activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',\n",
    "                                                            values=[0.01,0.001,0.0001])),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=[keras.metrics.categorical_accuracy])\n",
    "    return model\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective='val_categorical_accuracy',\n",
    "                   max_trials=5,\n",
    "                   overwrite=True,\n",
    "                   seed=99,\n",
    "                   executions_per_trial=5)\n",
    "\n",
    "#Keras cannot input object data type, so no matter the column is boolean or numeric we need to transform them to float32\n",
    "nn_dummy_x_train_no_out_float = np.asarray(nn_dummy_x_train_no_out).astype(np.float32)\n",
    "nn_dummy_x_val_no_out_float = np.asarray(nn_dummy_x_val_no_out).astype(np.float32)\n",
    "\n",
    "\n",
    "tuner.search(x=nn_dummy_x_train_no_out_float,\n",
    "             y=nn_dummy_y_train_no_out,\n",
    "             epochs=300,\n",
    "             batch_size=512,\n",
    "             validation_data=(nn_dummy_x_val_no_out_float,nn_dummy_y_val_no_out),\n",
    "             class_weight = nn_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 3 layers DNN parameters would be 140 neurons, relu as activation function, and 0.0001 learning rate.\n",
      "73/73 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      2150\n",
      "           1       0.38      0.70      0.49        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.67      0.48      0.56       128\n",
      "\n",
      "    accuracy                           0.95      2309\n",
      "   macro avg       0.51      0.54      0.51      2309\n",
      "weighted avg       0.95      0.95      0.95      2309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 3 layers DNN parameters would be {} neurons, {} as activation function, and {} learning rate.'.format(result['units'],result['activation'],result['learning_rate']))\n",
    "#print('------------------------------------------')\n",
    "#print(tuner.results_summary())\n",
    "\n",
    "#Get the final model\n",
    "Three_layers_NN=tuner.get_best_models()[0]\n",
    "\n",
    "#Get the predicton on validation data\n",
    "nn_y_val_pred=Three_layers_NN.predict(nn_dummy_x_val_no_out_float)\n",
    "nn_y_val_pred=np.argmax(nn_y_val_pred,axis=1)\n",
    "\n",
    "single_label_nn_dummy_y_val_no_out=np.argmax(nn_dummy_y_val_no_out,axis=1)\n",
    "print(classification_report(single_label_nn_dummy_y_val_no_out,nn_y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 31s]\n",
      "val_categorical_accuracy: 0.05543525516986847\n",
      "\n",
      "Best val_categorical_accuracy So Far: 0.9113613367080688\n",
      "Total elapsed time: 00h 08m 42s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Random Search on 10 Layers Neural Network\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid']),\n",
    "                           input_dim=nn_dummy_x_train_no_out.shape[1]))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=10,\n",
    "                                        max_value=150,\n",
    "                                        step=10),\n",
    "                           activation=hp.Choice('activation',values = ['relu','sigmoid'])))\n",
    "    model.add(layers.Dense(4,activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',\n",
    "                                                            values=[0.01,0.001,0.0001])),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=[keras.metrics.categorical_accuracy])\n",
    "    return model\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective='val_categorical_accuracy',\n",
    "                   max_trials=5,\n",
    "                   overwrite=True,\n",
    "                   seed=99,\n",
    "                   executions_per_trial=3)\n",
    "\n",
    "#Keras cannot input object data type, so no matter the column is boolean or numeric we need to transform them to float32\n",
    "nn_dummy_x_train_no_out_float = np.asarray(nn_dummy_x_train_no_out).astype(np.float32)\n",
    "nn_dummy_x_val_no_out_float = np.asarray(nn_dummy_x_val_no_out).astype(np.float32)\n",
    "\n",
    "\n",
    "tuner.search(x=nn_dummy_x_train_no_out_float,\n",
    "             y=nn_dummy_y_train_no_out,\n",
    "             epochs=400,\n",
    "             batch_size=512,\n",
    "             validation_data=(nn_dummy_x_val_no_out_float,nn_dummy_y_val_no_out),\n",
    "             class_weight = nn_class_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 10 layers DNN parameters would be 70 neurons, relu as activation function, and 0.0001 learning rate.\n",
      "73/73 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      2150\n",
      "           1       0.70      0.26      0.38        27\n",
      "           2       0.08      0.25      0.12         4\n",
      "           3       0.41      0.87      0.55       128\n",
      "\n",
      "    accuracy                           0.92      2309\n",
      "   macro avg       0.54      0.58      0.50      2309\n",
      "weighted avg       0.96      0.92      0.93      2309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 10 layers DNN parameters would be {} neurons, {} as activation function, and {} learning rate.'.format(result['units'],result['activation'],result['learning_rate']))\n",
    "#print('------------------------------------------')\n",
    "#print(tuner.results_summary())\n",
    "\n",
    "#Get the final model\n",
    "Ten_layers_NN=tuner.get_best_models()[0]\n",
    "\n",
    "#Get the predicton on validation data\n",
    "nn_y_val_pred=Ten_layers_NN.predict(nn_dummy_x_val_no_out_float)\n",
    "nn_y_val_pred=np.argmax(nn_y_val_pred,axis=1)\n",
    "\n",
    "single_label_nn_dummy_y_val_no_out=np.argmax(nn_dummy_y_val_no_out,axis=1)\n",
    "print(classification_report(single_label_nn_dummy_y_val_no_out,nn_y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the final results of the 3 layers NN model and 10 layers model. Even though it may look like the 3 layers NN model did a slightly better job, Yet, the **3 layers NN model actually did not capture any Platinum customers.** So, I believe that the **10 layers NN model with 70 neurons, relu as activation function, and 0.0001 learning rate did a better job.** However, comparing to our XGBoost model, it is still worse. So, I chose **the XGBoost model as the final model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model on Test Data ##\n",
    "After finding the best performing machine learning model, I will finally evaluate our XGBoost model's performance on the test data set to see the unbiased performance. Eventually, the classifier still did a **poor job on capturing the gold and platium customers**. **Less than 40% of the gold customers are captured, and none of the 3 platium customers are captured**. For further research, I believe with **a lot more sample, much deeper NN model, and more extreme sample weights**, we might be able to **scarifice some accuracy on the majority categories (silver and blue) to boost up performance on the important gold and platimnum customers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1879\n",
      "           1       0.53      0.38      0.44        26\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.73      0.94      0.82       118\n",
      "\n",
      "    accuracy                           0.97      2026\n",
      "   macro avg       0.56      0.58      0.56      2026\n",
      "weighted avg       0.97      0.97      0.97      2026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Load the final XGBoost model\n",
    "import joblib\n",
    "xgb_model = joblib.load(\"Final_XGBoost.pkl\")\n",
    "\n",
    "#Predict on test data\n",
    "y_test_pred = xgb_model.predict(dummy_x_test)\n",
    "\n",
    "#Final classification report on test data\n",
    "print(classification_report(single_label_y_test,y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
